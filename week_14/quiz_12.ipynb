{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jagz0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jagz0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jagz0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jagz0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jagz0\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached torch-2.5.1-cp39-cp39-win_amd64.whl (203.0 MB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 699.0 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 884.1 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 884.1 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 884.1 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 671.6 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 1.3/6.2 MB 330.7 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 1.3/6.2 MB 330.7 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 1.3/6.2 MB 330.7 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 1.3/6.2 MB 330.7 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 327.7 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 327.7 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 327.7 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 327.7 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 318.6 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 318.6 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 2.1/6.2 MB 341.4 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 2.4/6.2 MB 294.3 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 2.4/6.2 MB 294.3 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 2.4/6.2 MB 294.3 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 2.4/6.2 MB 294.3 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 2.4/6.2 MB 294.3 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 282.2 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 282.2 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 2.9/6.2 MB 299.1 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 3.1/6.2 MB 264.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 3.1/6.2 MB 264.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 3.1/6.2 MB 264.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 3.1/6.2 MB 264.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 3.1/6.2 MB 264.0 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 266.0 kB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 278.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 278.9 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 3.9/6.2 MB 290.7 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 3.9/6.2 MB 290.7 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 4.2/6.2 MB 302.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 4.2/6.2 MB 302.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 4.2/6.2 MB 302.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 4.2/6.2 MB 302.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 4.2/6.2 MB 302.1 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 298.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 298.3 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 306.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 289.6 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 279.1 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 252.1 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 252.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 5.8/6.2 MB 258.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 258.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 258.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 258.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 258.5 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.2 MB 259.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 259.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 259.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 259.6 kB/s eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\jagz0\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\Lib\\\\site-packages\\\\sympy\\\\polys\\\\domains\\\\integerring.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss 0.7325448393821716\n",
      "Epoch number: 11 with loss 0.6600155830383301\n",
      "Epoch number: 21 with loss 0.6357855200767517\n",
      "Epoch number: 31 with loss 0.6232737302780151\n",
      "Epoch number: 41 with loss 0.6109458208084106\n",
      "Epoch number: 51 with loss 0.5912511348724365\n",
      "Epoch number: 61 with loss 0.5709356665611267\n",
      "Epoch number: 71 with loss 0.549248218536377\n",
      "Epoch number: 81 with loss 0.5332964658737183\n",
      "Epoch number: 91 with loss 0.5167083144187927\n",
      "Epoch number: 101 with loss 0.504883885383606\n",
      "Epoch number: 111 with loss 0.48817214369773865\n",
      "Epoch number: 121 with loss 0.4804435670375824\n",
      "Epoch number: 131 with loss 0.4685402512550354\n",
      "Epoch number: 141 with loss 0.4585517346858978\n",
      "Epoch number: 151 with loss 0.44937705993652344\n",
      "Epoch number: 161 with loss 0.43556007742881775\n",
      "Epoch number: 171 with loss 0.42657989263534546\n",
      "Epoch number: 181 with loss 0.41821447014808655\n",
      "Epoch number: 191 with loss 0.4164431393146515\n",
      "Epoch number: 201 with loss 0.40306031703948975\n",
      "Epoch number: 211 with loss 0.3986617624759674\n",
      "Epoch number: 221 with loss 0.39796385169029236\n",
      "Epoch number: 231 with loss 0.38905081152915955\n",
      "Epoch number: 241 with loss 0.408007949590683\n",
      "Epoch number: 251 with loss 0.38805943727493286\n",
      "Epoch number: 261 with loss 0.3750874400138855\n",
      "Epoch number: 271 with loss 0.38256457448005676\n",
      "Epoch number: 281 with loss 0.37303683161735535\n",
      "Epoch number: 291 with loss 0.3593390882015228\n",
      "Epoch number: 301 with loss 0.3971731960773468\n",
      "Epoch number: 311 with loss 0.36790692806243896\n",
      "Epoch number: 321 with loss 0.3610958456993103\n",
      "Epoch number: 331 with loss 0.34893110394477844\n",
      "Epoch number: 341 with loss 0.34721124172210693\n",
      "Epoch number: 351 with loss 0.3435044586658478\n",
      "Epoch number: 361 with loss 0.3490099608898163\n",
      "Epoch number: 371 with loss 0.3407801687717438\n",
      "Epoch number: 381 with loss 0.34428542852401733\n",
      "Epoch number: 391 with loss 0.34052973985671997\n",
      "Epoch number: 401 with loss 0.3357737958431244\n",
      "Epoch number: 411 with loss 0.3379136025905609\n",
      "Epoch number: 421 with loss 0.35634374618530273\n",
      "Epoch number: 431 with loss 0.3315540850162506\n",
      "Epoch number: 441 with loss 0.3264777958393097\n",
      "Epoch number: 451 with loss 0.3293074369430542\n",
      "Epoch number: 461 with loss 0.3176810145378113\n",
      "Epoch number: 471 with loss 0.31573498249053955\n",
      "Epoch number: 481 with loss 0.34495267271995544\n",
      "Epoch number: 491 with loss 0.32253167033195496\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "ann = ANN_Model()\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(ann.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "\n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss {loss}')\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss 1.1355786323547363\n",
      "Epoch number: 11 with loss 0.679445207118988\n",
      "Epoch number: 21 with loss 0.6315260529518127\n",
      "Epoch number: 31 with loss 0.60909104347229\n",
      "Epoch number: 41 with loss 0.5900014638900757\n",
      "Epoch number: 51 with loss 0.5762112140655518\n",
      "Epoch number: 61 with loss 0.5692095160484314\n",
      "Epoch number: 71 with loss 0.5635229349136353\n",
      "Epoch number: 81 with loss 0.558547854423523\n",
      "Epoch number: 91 with loss 0.5541164875030518\n",
      "Epoch number: 101 with loss 0.5805302262306213\n",
      "Epoch number: 111 with loss 0.5524958372116089\n",
      "Epoch number: 121 with loss 0.5420418381690979\n",
      "Epoch number: 131 with loss 0.5323833227157593\n",
      "Epoch number: 141 with loss 0.5226786732673645\n",
      "Epoch number: 151 with loss 0.5162076950073242\n",
      "Epoch number: 161 with loss 0.5478731393814087\n",
      "Epoch number: 171 with loss 0.5179917812347412\n",
      "Epoch number: 181 with loss 0.5103484988212585\n",
      "Epoch number: 191 with loss 0.5047476887702942\n",
      "Epoch number: 201 with loss 0.5013521909713745\n",
      "Epoch number: 211 with loss 0.4977932572364807\n",
      "Epoch number: 221 with loss 0.5040541887283325\n",
      "Epoch number: 231 with loss 0.4970102906227112\n",
      "Epoch number: 241 with loss 0.4913337528705597\n",
      "Epoch number: 251 with loss 0.4976923167705536\n",
      "Epoch number: 261 with loss 0.4885347783565521\n",
      "Epoch number: 271 with loss 0.48726171255111694\n",
      "Epoch number: 281 with loss 0.4822657108306885\n",
      "Epoch number: 291 with loss 0.47924667596817017\n",
      "Epoch number: 301 with loss 0.4768490791320801\n",
      "Epoch number: 311 with loss 0.4753120243549347\n",
      "Epoch number: 321 with loss 0.48320066928863525\n",
      "Epoch number: 331 with loss 0.47315526008605957\n",
      "Epoch number: 341 with loss 0.4723745286464691\n",
      "Epoch number: 351 with loss 0.467357337474823\n",
      "Epoch number: 361 with loss 0.46868059039115906\n",
      "Epoch number: 371 with loss 0.4787629544734955\n",
      "Epoch number: 381 with loss 0.4594708979129791\n",
      "Epoch number: 391 with loss 0.45429643988609314\n",
      "Epoch number: 401 with loss 0.44902434945106506\n",
      "Epoch number: 411 with loss 0.4460589289665222\n",
      "Epoch number: 421 with loss 0.4778066873550415\n",
      "Epoch number: 431 with loss 0.4519840478897095\n",
      "Epoch number: 441 with loss 0.44416090846061707\n",
      "Epoch number: 451 with loss 0.4377748966217041\n",
      "Epoch number: 461 with loss 0.4902023375034332\n",
      "Epoch number: 471 with loss 0.43969470262527466\n",
      "Epoch number: 481 with loss 0.43741387128829956\n",
      "Epoch number: 491 with loss 0.4311334192752838\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "ann = ANN_Model()\n",
    "\n",
    " \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(ann.parameters(), lr = 0.01,momentum=0.9)\n",
    "\n",
    "\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "\n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss {loss}')\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer works by adjusting the learning rate between epochs, and also adjusting the gradient to improve convergeance. I compared SGD with Adam. By looking at the results, I can tell that Adam had a better performance because the loss numbers are less on the Adam epoch output. I think Adam is better suited for this database because it is more robust and stable than SGD, but it could also depend on the type of data used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 8 divisors: 1, 2, 4, 5, 8, 10, 20 and 40\n"
     ]
    }
   ],
   "source": [
    "def listing_and_counting(value : int = 1) -> str:\n",
    "    divisors: list[int] = []\n",
    "    for i in range (1, value + 1):\n",
    "        if value % i == 0:\n",
    "            divisors.append(i)\n",
    "\n",
    "    if len(divisors) == 2:\n",
    "        return f\"there are {len(divisors)} divisors: {divisors[0]} and {divisors[1]}\" \n",
    "    else:\n",
    "        return f\"there are {len(divisors)} divisors: {', '.join(map(str,divisors[:-1]))} and {divisors[-1]}\" \n",
    "    \n",
    "print(listing_and_counting(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
